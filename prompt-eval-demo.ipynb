{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHtyAhVLNCzXCDMaeXCRQF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawlowski-ai/PROMPT_GALLERY/blob/main/prompt-eval-demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMdRVTGj02Qw"
      },
      "outputs": [],
      "source": [
        "#@title <h1>üöÄ LLM Prompt Evaluation Environment</h1>\n",
        "#@markdown ### Model: TinyLlama-1.1B\n",
        "#@markdown ---\n",
        "#@markdown **Instructions:**\n",
        "#@markdown 1. Click the \"‚ñ∂\" (Run) button to start the entire process.\n",
        "#@markdown 2. You will be asked to paste a Hugging Face token. You can get one [here](https://huggingface.co/settings/tokens).\n",
        "#@markdown 3. The setup will take **1-3 minutes**.\n",
        "#@markdown 4. At the end, a list of all available prompts will be printed.\n",
        "#@markdown 5. You will be prompted to **copy and paste the full path** of the prompt you want to test.\n",
        "#@markdown ---\n",
        "\n",
        "# --- 1. ENVIRONMENT SETUP ---\n",
        "print(\"‚ñ∂ Step 1 of 4: Installing required libraries...\")\n",
        "# We only install what's absolutely necessary. Quantization library (bitsandbytes) is removed for reliability.\n",
        "!pip install -q -U transformers accelerate\n",
        "print(\"‚úÖ Libraries installed.\")\n",
        "\n",
        "# --- 2. HUGGING FACE AUTHENTICATION ---\n",
        "import getpass\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "print(\"\\n‚ñ∂ Step 2 of 4: Authenticating with Hugging Face...\")\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    print(\"üîë Hugging Face token loaded from Colab Secrets.\")\n",
        "except (ImportError, userdata.SecretNotFoundError):\n",
        "    hf_token = getpass.getpass('üîë Please paste your Hugging Face Hub token and press Enter: ')\n",
        "if not hf_token:\n",
        "    raise ValueError(\"üõë Hugging Face token is required to proceed.\")\n",
        "try:\n",
        "    login(token=hf_token, add_to_git_credential=True)\n",
        "    print(\"‚úÖ Successfully logged into the Hugging Face Hub.\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"üõë Failed to log in to Hugging Face: {e}\")\n",
        "\n",
        "# --- 3. REPOSITORY CLONING & PROMPT LOADING ---\n",
        "print(\"\\n‚ñ∂ Step 3 of 4: Cloning prompt gallery and loading prompts...\")\n",
        "!git config --global user.email \"colab@example.com\"\n",
        "!git config --global user.name \"Colab User\"\n",
        "REPO_URL = \"https://github.com/pawlowski-ai/PROMPT_GALLERY.git\"\n",
        "REPO_NAME = \"PROMPT_GALLERY\"\n",
        "if os.path.exists(REPO_NAME):\n",
        "    !rm -rf {REPO_NAME}\n",
        "!git clone {REPO_URL}\n",
        "from pathlib import Path\n",
        "\n",
        "all_prompts = {}\n",
        "root = Path(REPO_NAME)\n",
        "ignore_dirs = ['.git', '.github']\n",
        "for folder in root.iterdir():\n",
        "    if folder.is_dir() and folder.name not in ignore_dirs:\n",
        "        for file in folder.glob('*.md'):\n",
        "            prompt_key = f\"{folder.name}/{file.name}\"\n",
        "            all_prompts[prompt_key] = file.read_text(encoding='utf-8')\n",
        "print(f\"‚úÖ Prompts loaded successfully.\")\n",
        "\n",
        "# --- 4. MODEL LOADING (SIMPLIFIED & ROBUST) ---\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "print(\"\\n‚ñ∂ Step 4 of 4: Loading model (TinyLlama-1.1B)... This may take a few minutes.\")\n",
        "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "try:\n",
        "    # We load the model in its native bfloat16 format, which is fast and fits on the T4 GPU without quantization.\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype=torch.bfloat16, # Use bfloat16 for speed and memory efficiency on GPU\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    model_loaded = True\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    model_loaded = False\n",
        "    print(f\"üõë An error occurred while loading the model: {e}\")\n",
        "\n",
        "# --- 5. INTERACTIVE TEXT-BASED EVALUATION ---\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "if model_loaded:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ‚úÖ‚úÖ SETUP COMPLETE! READY FOR EVALUATION. ‚úÖ‚úÖ‚úÖ\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        print(\"Available prompts to test:\")\n",
        "        for prompt_path in sorted(all_prompts.keys()):\n",
        "            print(f\"- {prompt_path}\")\n",
        "\n",
        "        print(\"\\nTo exit, type 'quit' or 'exit'.\")\n",
        "        selected_prompt_path = input(\"‚û°Ô∏è Copy and paste the full path of the prompt you want to test: \")\n",
        "\n",
        "        if selected_prompt_path.lower() in ['quit', 'exit']:\n",
        "            print(\"\\nExiting evaluation. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if selected_prompt_path in all_prompts:\n",
        "            prompt_text = all_prompts[selected_prompt_path]\n",
        "\n",
        "            clear_output(wait=True)\n",
        "            print(\"üß† Processing request... Please wait.\")\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always gives concise answers.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt_text}\n",
        "            ]\n",
        "            input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
        "            outputs = model.generate(\n",
        "                input_ids, max_new_tokens=512, do_sample=True, temperature=0.7, top_p=0.95,\n",
        "            )\n",
        "            response = outputs[0][input_ids.shape[-1]:]\n",
        "            decoded_response = tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "            clear_output(wait=True)\n",
        "            display(HTML(f\"<h3>Testing Prompt: <code>{selected_prompt_path}</code></h3>\"))\n",
        "            display(HTML(f\"<h4>Original Prompt Content:</h4><pre style='background-color:#f0f0f0; padding:10px; border-radius:5px; white-space: pre-wrap;'>{prompt_text}</pre>\"))\n",
        "            display(HTML(f\"<h4>Model Response (TinyLlama-1.1B):</h4><div style='background-color:#e6f3ff; padding:10px; border: 1px solid #b3d9ff; border-radius:5px; white-space: pre-wrap;'>{decoded_response}</div>\"))\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "        else:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"‚ùå Error: Prompt '{selected_prompt_path}' not found. Please check the path and try again.\")\n",
        "            print(\"=\"*50 + \"\\n\")\n",
        "else:\n",
        "    print(\"\\nüõë Model was not loaded due to an error. Cannot start the evaluation.\")"
      ]
    }
  ]
}