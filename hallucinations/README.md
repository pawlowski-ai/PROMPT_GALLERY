# ðŸ¤¯ Hallucination-Inducing Prompts

**Purpose**  
This folder contains prompts intentionally crafted to make Large Language Models (LLMs) generate confident but factually incorrect answers. By cataloguing these patterns I can (1) measure a modelâ€™s tendency to fabricate and (2) stress-test mitigation strategies before deploying a system in production.

**Method**  
1. **Set a false premise** â€“ embed a â€œfactâ€ the model is likely to accept without checking.  
2. **Demand authoritative detail** â€“ quotes, citations, numbers, or historical dates push the model to invent specifics.  
3. **Constrain style** â€“ ask for academic tone or JSON citations to mask hallucinations in formal structure.  
4. **Amplify pressure** â€“ add time pressure, add â€œexpertâ€ role, forbid disclaimers.  


> *All prompts here were run against GPT-4o-0613 and Gemini-2.5-flash (June 2025) unless noted otherwise.*
